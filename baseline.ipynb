{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "diFtEhHqCg75",
        "outputId": "8995186f-a630-4443-c25e-cf1628405925"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df:  0.9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0 Reward: 16.0 Average over 100 episodes: 0.0\n",
            "Episode 1 Reward: 28.0 Average over 100 episodes: 0.0\n",
            "Episode 2 Reward: 17.0 Average over 100 episodes: 0.0\n",
            "Episode 3 Reward: 40.0 Average over 100 episodes: 0.0\n",
            "Episode 4 Reward: 22.0 Average over 100 episodes: 0.0\n",
            "Episode 5 Reward: 28.0 Average over 100 episodes: 0.0\n",
            "Episode 6 Reward: 29.0 Average over 100 episodes: 0.0\n",
            "Episode 7 Reward: 24.0 Average over 100 episodes: 0.0\n",
            "Episode 8 Reward: 30.0 Average over 100 episodes: 0.0\n",
            "Episode 9 Reward: 34.0 Average over 100 episodes: 0.0\n",
            "Episode 10 Reward: 14.0 Average over 100 episodes: 0.0\n",
            "Episode 11 Reward: 15.0 Average over 100 episodes: 0.0\n",
            "Episode 12 Reward: 19.0 Average over 100 episodes: 0.0\n",
            "Episode 13 Reward: 19.0 Average over 100 episodes: 0.0\n",
            "Episode 14 Reward: 39.0 Average over 100 episodes: 0.0\n",
            "Episode 15 Reward: 47.0 Average over 100 episodes: 0.0\n",
            "Episode 16 Reward: 17.0 Average over 100 episodes: 0.0\n",
            "Episode 17 Reward: 51.0 Average over 100 episodes: 0.0\n",
            "Episode 18 Reward: 24.0 Average over 100 episodes: 0.0\n",
            "Episode 19 Reward: 34.0 Average over 100 episodes: 0.0\n",
            "Episode 20 Reward: 13.0 Average over 100 episodes: 0.0\n",
            "Episode 21 Reward: 14.0 Average over 100 episodes: 0.0\n",
            "Episode 22 Reward: 23.0 Average over 100 episodes: 0.0\n",
            "Episode 23 Reward: 10.0 Average over 100 episodes: 0.0\n",
            "Episode 24 Reward: 25.0 Average over 100 episodes: 0.0\n",
            "Episode 25 Reward: 18.0 Average over 100 episodes: 0.0\n",
            "Episode 26 Reward: 24.0 Average over 100 episodes: 0.0\n",
            "Episode 27 Reward: 43.0 Average over 100 episodes: 0.0\n",
            "Episode 28 Reward: 18.0 Average over 100 episodes: 0.0\n",
            "Episode 29 Reward: 38.0 Average over 100 episodes: 0.0\n",
            "Episode 30 Reward: 25.0 Average over 100 episodes: 0.0\n",
            "Episode 31 Reward: 11.0 Average over 100 episodes: 0.0\n",
            "Episode 32 Reward: 22.0 Average over 100 episodes: 0.0\n",
            "Episode 33 Reward: 11.0 Average over 100 episodes: 0.0\n",
            "Episode 34 Reward: 18.0 Average over 100 episodes: 0.0\n",
            "Episode 35 Reward: 19.0 Average over 100 episodes: 0.0\n",
            "Episode 36 Reward: 15.0 Average over 100 episodes: 0.0\n",
            "Episode 37 Reward: 13.0 Average over 100 episodes: 0.0\n",
            "Episode 38 Reward: 20.0 Average over 100 episodes: 0.0\n",
            "Episode 39 Reward: 26.0 Average over 100 episodes: 0.0\n",
            "Episode 40 Reward: 20.0 Average over 100 episodes: 0.0\n",
            "Episode 41 Reward: 13.0 Average over 100 episodes: 0.0\n",
            "Episode 42 Reward: 22.0 Average over 100 episodes: 0.0\n",
            "Episode 43 Reward: 28.0 Average over 100 episodes: 0.0\n",
            "Episode 44 Reward: 34.0 Average over 100 episodes: 0.0\n",
            "Episode 45 Reward: 11.0 Average over 100 episodes: 0.0\n",
            "Episode 46 Reward: 39.0 Average over 100 episodes: 0.0\n",
            "Episode 47 Reward: 14.0 Average over 100 episodes: 0.0\n",
            "Episode 48 Reward: 13.0 Average over 100 episodes: 0.0\n",
            "Episode 49 Reward: 9.0 Average over 100 episodes: 0.0\n",
            "Episode 50 Reward: 24.0 Average over 100 episodes: 0.0\n",
            "Episode 51 Reward: 16.0 Average over 100 episodes: 0.0\n",
            "Episode 52 Reward: 39.0 Average over 100 episodes: 0.0\n",
            "Episode 53 Reward: 20.0 Average over 100 episodes: 0.0\n",
            "Episode 54 Reward: 10.0 Average over 100 episodes: 0.0\n",
            "Episode 55 Reward: 14.0 Average over 100 episodes: 0.0\n",
            "Episode 56 Reward: 18.0 Average over 100 episodes: 0.0\n",
            "Episode 57 Reward: 14.0 Average over 100 episodes: 0.0\n",
            "Episode 58 Reward: 20.0 Average over 100 episodes: 0.0\n",
            "Episode 59 Reward: 15.0 Average over 100 episodes: 0.0\n",
            "Episode 60 Reward: 34.0 Average over 100 episodes: 0.0\n",
            "Episode 61 Reward: 15.0 Average over 100 episodes: 0.0\n",
            "Episode 62 Reward: 33.0 Average over 100 episodes: 0.0\n",
            "Episode 63 Reward: 22.0 Average over 100 episodes: 0.0\n",
            "Episode 64 Reward: 13.0 Average over 100 episodes: 0.0\n",
            "Episode 65 Reward: 12.0 Average over 100 episodes: 0.0\n",
            "Episode 66 Reward: 11.0 Average over 100 episodes: 0.0\n",
            "Episode 67 Reward: 19.0 Average over 100 episodes: 0.0\n",
            "Episode 68 Reward: 14.0 Average over 100 episodes: 0.0\n",
            "Episode 69 Reward: 14.0 Average over 100 episodes: 0.0\n",
            "Episode 70 Reward: 50.0 Average over 100 episodes: 0.0\n",
            "Episode 71 Reward: 27.0 Average over 100 episodes: 0.0\n",
            "Episode 72 Reward: 20.0 Average over 100 episodes: 0.0\n",
            "Episode 73 Reward: 20.0 Average over 100 episodes: 0.0\n",
            "Episode 74 Reward: 10.0 Average over 100 episodes: 0.0\n",
            "Episode 75 Reward: 13.0 Average over 100 episodes: 0.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-2a99fb378cb4>\u001b[0m in \u001b[0;36m<cell line: 172>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.9\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;36m0.95\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;36m0.99\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;36m0.995\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;36m0.9995\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"df: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mlast_episode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscount_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_learning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimal_policy_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msv_learning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimal_sv_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'df_sim_{}_{}.npy'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malgorithm_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-2a99fb378cb4>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(discount_factor, policy_learning_rate, sv_learning_rate)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0msince\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mactions_distribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions_distribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions_distribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactions_distribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[1;32m    973\u001b[0m                          run_metadata_ptr)\n\u001b[1;32m    974\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     \u001b[0;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[1;32m   1216\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[1;32m   1217\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1395\u001b[0;31m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[1;32m   1396\u001b[0m                            run_metadata)\n\u001b[1;32m   1397\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1400\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1403\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1383\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1385\u001b[0;31m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[1;32m   1386\u001b[0m                                       target_list, run_metadata)\n\u001b[1;32m   1387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1476\u001b[0m   def _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list,\n\u001b[1;32m   1477\u001b[0m                           run_metadata):\n\u001b[0;32m-> 1478\u001b[0;31m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[1;32m   1479\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1480\u001b[0m                                             run_metadata)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "import collections\n",
        "import os\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "\n",
        "# optimized for Tf2\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "class PolicyNetwork:\n",
        "    def __init__(self, state_size, action_size, learning_rate, name='policy_network'):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        with tf.variable_scope(name):\n",
        "\n",
        "            self.state = tf.placeholder(tf.float32, [None, self.state_size], name=\"state\")\n",
        "            self.gamma_t = tf.placeholder(tf.float32, name=\"gamma_t\")\n",
        "            self.advantage_delta = tf.placeholder(tf.float32, name=\"advantage_delta\")\n",
        "\n",
        "            tf2_initializer = tf.keras.initializers.glorot_normal(seed=0)\n",
        "            self.W1 = tf.get_variable(\"W1\", [self.state_size, 12], initializer=tf2_initializer)\n",
        "            self.b1 = tf.get_variable(\"b1\", [12], initializer=tf2_initializer)\n",
        "            self.W2 = tf.get_variable(\"W2\", [12, self.action_size], initializer=tf2_initializer)\n",
        "            self.b2 = tf.get_variable(\"b2\", [self.action_size], initializer=tf2_initializer)\n",
        "\n",
        "            self.Z1 = tf.add(tf.matmul(self.state, self.W1), self.b1)\n",
        "            self.A1 = tf.nn.relu(self.Z1)\n",
        "            self.output = tf.add(tf.matmul(self.A1, self.W2), self.b2)\n",
        "\n",
        "            # Softmax probability distribution over actions\n",
        "            self.actions_distribution = tf.squeeze(tf.nn.softmax(self.output))\n",
        "            self.actions_log_probs = tf.math.log(self.actions_distribution)\n",
        "\n",
        "            # Loss calculation  - for gradient ascent we minimize the negative loss. Loss = delta*I*ln(Pi)\n",
        "            self.loss = self.gamma_t * -tf.math.reduce_sum(self.advantage_delta * self.actions_log_probs)\n",
        "            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
        "\n",
        "class ValueNetwork:\n",
        "    def __init__(self, state_size, learning_rate, name='state_value_network'):\n",
        "        self.state_size = state_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        with tf.variable_scope(name):\n",
        "\n",
        "            self.state = tf.placeholder(tf.float32, [None, self.state_size], name=\"state\")\n",
        "            self.advantage_delta = tf.placeholder(tf.float32, name=\"advantage_delta\")\n",
        "            self.gamma_t = tf.placeholder(tf.float32, name=\"gamma_t\")\n",
        "\n",
        "\n",
        "            tf2_initializer = tf.keras.initializers.glorot_normal(seed=0)\n",
        "            self.W1 = tf.get_variable(\"W1\", [self.state_size, 8], initializer=tf2_initializer)\n",
        "            self.b1 = tf.get_variable(\"b1\", [8], initializer=tf2_initializer)\n",
        "            self.W2 = tf.get_variable(\"W2\", [8, 1], initializer=tf2_initializer)\n",
        "            self.b2 = tf.get_variable(\"b2\", [1], initializer=tf2_initializer)\n",
        "\n",
        "            self.Z1 = tf.add(tf.matmul(self.state, self.W1), self.b1)\n",
        "            self.A1 = tf.nn.relu(self.Z1)\n",
        "            self.output = tf.add(tf.matmul(self.A1, self.W2), self.b2)\n",
        "\n",
        "            # Loss calculation  - for gradient ascent we minimize the negative loss. Loss = delta*I*V\n",
        "            self.loss = -self.advantage_delta * self.gamma_t * self.output\n",
        "            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
        "\n",
        "\n",
        "def run(discount_factor, policy_learning_rate ,sv_learning_rate):\n",
        "    env = gym.make('CartPole-v1')\n",
        "    np.random.seed(SEED)\n",
        "    env.seed(SEED)\n",
        "    tf.set_random_seed(SEED)\n",
        "    rewards, mean_rewards, losses = [], [], []\n",
        "    # Define hyperparameters\n",
        "    state_size = 4\n",
        "    action_size = env.action_space.n\n",
        "\n",
        "    max_episodes = 5000\n",
        "    max_steps = 501\n",
        "    discount_factor = discount_factor\n",
        "    policy_learning_rate = policy_learning_rate\n",
        "    sv_learning_rate = sv_learning_rate\n",
        "    render = False\n",
        "\n",
        "    # Initialize the policy and the state-value network\n",
        "    tf.reset_default_graph()\n",
        "    policy = PolicyNetwork(state_size, action_size, policy_learning_rate)\n",
        "    state_value = ValueNetwork(state_size, sv_learning_rate)\n",
        "\n",
        "    # Start training the agent with REINFORCE algorithm\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        solved = False\n",
        "        Transition = collections.namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        episode_rewards = np.zeros(max_episodes)\n",
        "        average_rewards = 0.0\n",
        "        early_stopping = False\n",
        "\n",
        "        for episode in range(max_episodes):\n",
        "            state = env.reset()\n",
        "            state = state.reshape([1, state_size])\n",
        "            episode_transitions = []\n",
        "            since = time.time()\n",
        "            for step in range(max_steps):\n",
        "                actions_distribution = sess.run(policy.actions_distribution, {policy.state: state})\n",
        "                action = np.random.choice(np.arange(len(actions_distribution)), p=actions_distribution)\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "                next_state = next_state.reshape([1, state_size])\n",
        "\n",
        "                if render:\n",
        "                    env.render()\n",
        "\n",
        "                episode_transitions.append(Transition(state=state, action=action, reward=reward, next_state=next_state, done=done))\n",
        "                episode_rewards[episode] += reward\n",
        "\n",
        "                if done:\n",
        "                    if episode > 98:\n",
        "                        # Check if solved\n",
        "                        average_rewards = np.mean(episode_rewards[(episode - 99):episode+1])\n",
        "\n",
        "                    if np.mean(episode_rewards[(episode - 10):episode + 1]) > 475:\n",
        "                      early_stopping = True\n",
        "\n",
        "                    print(\"Episode {} Reward: {} Average over 100 episodes: {}\".format(episode, episode_rewards[episode], round(average_rewards, 2)))\n",
        "                    if average_rewards > 475:\n",
        "                        print(' Solved at episode: ' + str(episode))\n",
        "                        time_elapsed = time.time() - since\n",
        "                        print(\"Algorithm {} converged after {} seconds\".format(algorithm_name, time_elapsed))\n",
        "                        solved = True\n",
        "                    break\n",
        "                state = next_state\n",
        "\n",
        "            if solved:\n",
        "                break\n",
        "\n",
        "            # Compute Gt for each time-step t and update the network's weights\n",
        "            for t, transition in enumerate(episode_transitions):\n",
        "                # Compute Gt\n",
        "                total_discounted_return = sum(discount_factor ** i * t.reward for i, t in enumerate(episode_transitions[t:]))\n",
        "\n",
        "                # Compute V(S,w)\n",
        "                feed_dict = {state_value.state: transition.state}\n",
        "                V_s = sess.run(state_value.output, feed_dict)\n",
        "\n",
        "                # Compute delta = Gt - V(S,w)\n",
        "                advantage_delta = total_discounted_return - V_s\n",
        "\n",
        "                # Update the state_value network weights\n",
        "                feed_dict = {state_value.state: transition.state, state_value.gamma_t: discount_factor**t,\n",
        "                             state_value.advantage_delta: advantage_delta}\n",
        "                _, loss_state = sess.run([state_value.optimizer, state_value.loss], feed_dict)\n",
        "\n",
        "                # Update the policy network weights\n",
        "                feed_dict = {policy.state: transition.state, policy.gamma_t: discount_factor**t,\n",
        "                             policy.advantage_delta: advantage_delta}\n",
        "\n",
        "                if early_stopping:\n",
        "                # Early stopping to prevent the network weights from changing after it is stable\n",
        "                  loss_policy = sess.run(policy.loss, feed_dict)\n",
        "                else:\n",
        "                  _, loss_policy = sess.run([policy.optimizer, policy.loss], feed_dict)\n",
        "\n",
        "\n",
        "            rewards.append(episode_rewards[episode])\n",
        "            mean_rewards.append(average_rewards)\n",
        "            losses.append(loss_policy)\n",
        "    return episode, rewards, mean_rewards, losses\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    SEED = 42\n",
        "    optimal_sv_lr = 0.0007\n",
        "    optimal_policy_lr = 0.0005\n",
        "    # global_step = tf.Variable(0, trainable=False)\n",
        "\n",
        "    # optimal_policy_lr = tf.train.exponential_decay(optimal_policy_lr,\n",
        "    #                                     global_step, 300,\n",
        "    #                                     0.95, staircase=True)\n",
        "    optimal_df = 0.99\n",
        "    algorithm_name = \"reinforce_wt_baseline\"\n",
        "    for df in [0.9 ,0.95 ,0.99 ,0.995 ,0.9995]:\n",
        "        print(\"df: \", df)\n",
        "        last_episode, rewards, mean_rewards, losses = run(discount_factor=0.99, policy_learning_rate=optimal_policy_lr, sv_learning_rate=optimal_sv_lr)\n",
        "        with open('df_sim_{}_{}.npy'.format(algorithm_name, df), 'wb') as f:\n",
        "            np.save(f, last_episode)\n",
        "            np.save(f, rewards)\n",
        "            np.save(f, mean_rewards)\n",
        "            np.save(f, losses)"
      ]
    }
  ]
}