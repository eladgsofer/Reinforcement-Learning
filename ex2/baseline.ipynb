{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "diFtEhHqCg75",
    "outputId": "8995186f-a630-4443-c25e-cf1628405925"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001B[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n",
      "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001B[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n",
      "/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001B[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001B[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "df:  0.9\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Episode 0 Reward: 16.0 Average over 100 episodes: 0.0\n",
      "Episode 1 Reward: 28.0 Average over 100 episodes: 0.0\n",
      "Episode 2 Reward: 17.0 Average over 100 episodes: 0.0\n",
      "Episode 3 Reward: 40.0 Average over 100 episodes: 0.0\n",
      "Episode 4 Reward: 22.0 Average over 100 episodes: 0.0\n",
      "Episode 5 Reward: 28.0 Average over 100 episodes: 0.0\n",
      "Episode 6 Reward: 29.0 Average over 100 episodes: 0.0\n",
      "Episode 7 Reward: 24.0 Average over 100 episodes: 0.0\n",
      "Episode 8 Reward: 30.0 Average over 100 episodes: 0.0\n",
      "Episode 9 Reward: 34.0 Average over 100 episodes: 0.0\n",
      "Episode 10 Reward: 14.0 Average over 100 episodes: 0.0\n",
      "Episode 11 Reward: 15.0 Average over 100 episodes: 0.0\n",
      "Episode 12 Reward: 19.0 Average over 100 episodes: 0.0\n",
      "Episode 13 Reward: 19.0 Average over 100 episodes: 0.0\n",
      "Episode 14 Reward: 39.0 Average over 100 episodes: 0.0\n",
      "Episode 15 Reward: 47.0 Average over 100 episodes: 0.0\n",
      "Episode 16 Reward: 17.0 Average over 100 episodes: 0.0\n",
      "Episode 17 Reward: 51.0 Average over 100 episodes: 0.0\n",
      "Episode 18 Reward: 24.0 Average over 100 episodes: 0.0\n",
      "Episode 19 Reward: 34.0 Average over 100 episodes: 0.0\n",
      "Episode 20 Reward: 13.0 Average over 100 episodes: 0.0\n",
      "Episode 21 Reward: 14.0 Average over 100 episodes: 0.0\n",
      "Episode 22 Reward: 23.0 Average over 100 episodes: 0.0\n",
      "Episode 23 Reward: 10.0 Average over 100 episodes: 0.0\n",
      "Episode 24 Reward: 25.0 Average over 100 episodes: 0.0\n",
      "Episode 25 Reward: 18.0 Average over 100 episodes: 0.0\n",
      "Episode 26 Reward: 24.0 Average over 100 episodes: 0.0\n",
      "Episode 27 Reward: 43.0 Average over 100 episodes: 0.0\n",
      "Episode 28 Reward: 18.0 Average over 100 episodes: 0.0\n",
      "Episode 29 Reward: 38.0 Average over 100 episodes: 0.0\n",
      "Episode 30 Reward: 25.0 Average over 100 episodes: 0.0\n",
      "Episode 31 Reward: 11.0 Average over 100 episodes: 0.0\n",
      "Episode 32 Reward: 22.0 Average over 100 episodes: 0.0\n",
      "Episode 33 Reward: 11.0 Average over 100 episodes: 0.0\n",
      "Episode 34 Reward: 18.0 Average over 100 episodes: 0.0\n",
      "Episode 35 Reward: 19.0 Average over 100 episodes: 0.0\n",
      "Episode 36 Reward: 15.0 Average over 100 episodes: 0.0\n",
      "Episode 37 Reward: 13.0 Average over 100 episodes: 0.0\n",
      "Episode 38 Reward: 20.0 Average over 100 episodes: 0.0\n",
      "Episode 39 Reward: 26.0 Average over 100 episodes: 0.0\n",
      "Episode 40 Reward: 20.0 Average over 100 episodes: 0.0\n",
      "Episode 41 Reward: 13.0 Average over 100 episodes: 0.0\n",
      "Episode 42 Reward: 22.0 Average over 100 episodes: 0.0\n",
      "Episode 43 Reward: 28.0 Average over 100 episodes: 0.0\n",
      "Episode 44 Reward: 34.0 Average over 100 episodes: 0.0\n",
      "Episode 45 Reward: 11.0 Average over 100 episodes: 0.0\n",
      "Episode 46 Reward: 39.0 Average over 100 episodes: 0.0\n",
      "Episode 47 Reward: 14.0 Average over 100 episodes: 0.0\n",
      "Episode 48 Reward: 13.0 Average over 100 episodes: 0.0\n",
      "Episode 49 Reward: 9.0 Average over 100 episodes: 0.0\n",
      "Episode 50 Reward: 24.0 Average over 100 episodes: 0.0\n",
      "Episode 51 Reward: 16.0 Average over 100 episodes: 0.0\n",
      "Episode 52 Reward: 39.0 Average over 100 episodes: 0.0\n",
      "Episode 53 Reward: 20.0 Average over 100 episodes: 0.0\n",
      "Episode 54 Reward: 10.0 Average over 100 episodes: 0.0\n",
      "Episode 55 Reward: 14.0 Average over 100 episodes: 0.0\n",
      "Episode 56 Reward: 18.0 Average over 100 episodes: 0.0\n",
      "Episode 57 Reward: 14.0 Average over 100 episodes: 0.0\n",
      "Episode 58 Reward: 20.0 Average over 100 episodes: 0.0\n",
      "Episode 59 Reward: 15.0 Average over 100 episodes: 0.0\n",
      "Episode 60 Reward: 34.0 Average over 100 episodes: 0.0\n",
      "Episode 61 Reward: 15.0 Average over 100 episodes: 0.0\n",
      "Episode 62 Reward: 33.0 Average over 100 episodes: 0.0\n",
      "Episode 63 Reward: 22.0 Average over 100 episodes: 0.0\n",
      "Episode 64 Reward: 13.0 Average over 100 episodes: 0.0\n",
      "Episode 65 Reward: 12.0 Average over 100 episodes: 0.0\n",
      "Episode 66 Reward: 11.0 Average over 100 episodes: 0.0\n",
      "Episode 67 Reward: 19.0 Average over 100 episodes: 0.0\n",
      "Episode 68 Reward: 14.0 Average over 100 episodes: 0.0\n",
      "Episode 69 Reward: 14.0 Average over 100 episodes: 0.0\n",
      "Episode 70 Reward: 50.0 Average over 100 episodes: 0.0\n",
      "Episode 71 Reward: 27.0 Average over 100 episodes: 0.0\n",
      "Episode 72 Reward: 20.0 Average over 100 episodes: 0.0\n",
      "Episode 73 Reward: 20.0 Average over 100 episodes: 0.0\n",
      "Episode 74 Reward: 10.0 Average over 100 episodes: 0.0\n",
      "Episode 75 Reward: 13.0 Average over 100 episodes: 0.0\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-14-2a99fb378cb4>\u001B[0m in \u001B[0;36m<cell line: 172>\u001B[0;34m()\u001B[0m\n\u001B[1;32m    183\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mdf\u001B[0m \u001B[0;32min\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;36m0.9\u001B[0m \u001B[0;34m,\u001B[0m\u001B[0;36m0.95\u001B[0m \u001B[0;34m,\u001B[0m\u001B[0;36m0.99\u001B[0m \u001B[0;34m,\u001B[0m\u001B[0;36m0.995\u001B[0m \u001B[0;34m,\u001B[0m\u001B[0;36m0.9995\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    184\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"df: \"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 185\u001B[0;31m         \u001B[0mlast_episode\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrewards\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmean_rewards\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlosses\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdiscount_factor\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.99\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpolicy_learning_rate\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moptimal_policy_lr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msv_learning_rate\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moptimal_sv_lr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    186\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'df_sim_{}_{}.npy'\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0malgorithm_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'wb'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    187\u001B[0m             \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msave\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlast_episode\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-14-2a99fb378cb4>\u001B[0m in \u001B[0;36mrun\u001B[0;34m(discount_factor, policy_learning_rate, sv_learning_rate)\u001B[0m\n\u001B[1;32m    105\u001B[0m             \u001B[0msince\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    106\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mstep\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmax_steps\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 107\u001B[0;31m                 \u001B[0mactions_distribution\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msess\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpolicy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mactions_distribution\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0mpolicy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstate\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    108\u001B[0m                 \u001B[0maction\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrandom\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mchoice\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mactions_distribution\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mp\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mactions_distribution\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    109\u001B[0m                 \u001B[0mnext_state\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(self, fetches, feed_dict, options, run_metadata)\u001B[0m\n\u001B[1;32m    970\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    971\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 972\u001B[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001B[0m\u001B[1;32m    973\u001B[0m                          run_metadata_ptr)\n\u001B[1;32m    974\u001B[0m       \u001B[0;32mif\u001B[0m \u001B[0mrun_metadata\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001B[0m in \u001B[0;36m_run\u001B[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001B[0m\n\u001B[1;32m   1213\u001B[0m     \u001B[0;31m# or if the call is a partial run that specifies feeds.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1214\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mfinal_fetches\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mfinal_targets\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mhandle\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mfeed_dict_tensor\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1215\u001B[0;31m       results = self._do_run(handle, final_targets, final_fetches,\n\u001B[0m\u001B[1;32m   1216\u001B[0m                              feed_dict_tensor, options, run_metadata)\n\u001B[1;32m   1217\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001B[0m in \u001B[0;36m_do_run\u001B[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001B[0m\n\u001B[1;32m   1393\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1394\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mhandle\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1395\u001B[0;31m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001B[0m\u001B[1;32m   1396\u001B[0m                            run_metadata)\n\u001B[1;32m   1397\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001B[0m in \u001B[0;36m_do_call\u001B[0;34m(self, fn, *args)\u001B[0m\n\u001B[1;32m   1400\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m_do_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1401\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1402\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1403\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0merrors\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mOpError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1404\u001B[0m       \u001B[0mmessage\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcompat\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mas_text\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmessage\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001B[0m in \u001B[0;36m_run_fn\u001B[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001B[0m\n\u001B[1;32m   1383\u001B[0m       \u001B[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1384\u001B[0m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_extend_graph\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1385\u001B[0;31m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001B[0m\u001B[1;32m   1386\u001B[0m                                       target_list, run_metadata)\n\u001B[1;32m   1387\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001B[0m in \u001B[0;36m_call_tf_sessionrun\u001B[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001B[0m\n\u001B[1;32m   1476\u001B[0m   def _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list,\n\u001B[1;32m   1477\u001B[0m                           run_metadata):\n\u001B[0;32m-> 1478\u001B[0;31m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001B[0m\u001B[1;32m   1479\u001B[0m                                             \u001B[0mfetch_list\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget_list\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1480\u001B[0m                                             run_metadata)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "import collections\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "\n",
    "# optimized for Tf2\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "class PolicyNetwork:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name='policy_network'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "\n",
    "            self.state = tf.placeholder(tf.float32, [None, self.state_size], name=\"state\")\n",
    "            self.gamma_t = tf.placeholder(tf.float32, name=\"gamma_t\")\n",
    "            self.advantage_delta = tf.placeholder(tf.float32, name=\"advantage_delta\")\n",
    "\n",
    "            tf2_initializer = tf.keras.initializers.glorot_normal(seed=0)\n",
    "            self.W1 = tf.get_variable(\"W1\", [self.state_size, 12], initializer=tf2_initializer)\n",
    "            self.b1 = tf.get_variable(\"b1\", [12], initializer=tf2_initializer)\n",
    "            self.W2 = tf.get_variable(\"W2\", [12, self.action_size], initializer=tf2_initializer)\n",
    "            self.b2 = tf.get_variable(\"b2\", [self.action_size], initializer=tf2_initializer)\n",
    "\n",
    "            self.Z1 = tf.add(tf.matmul(self.state, self.W1), self.b1)\n",
    "            self.A1 = tf.nn.relu(self.Z1)\n",
    "            self.output = tf.add(tf.matmul(self.A1, self.W2), self.b2)\n",
    "\n",
    "            # Softmax probability distribution over actions\n",
    "            self.actions_distribution = tf.squeeze(tf.nn.softmax(self.output))\n",
    "            self.actions_log_probs = tf.math.log(self.actions_distribution)\n",
    "\n",
    "            # Loss calculation  - for gradient ascent we minimize the negative loss. Loss = delta*I*ln(Pi)\n",
    "            self.loss = self.gamma_t * -tf.math.reduce_sum(self.advantage_delta * self.actions_log_probs)\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "\n",
    "class ValueNetwork:\n",
    "    def __init__(self, state_size, learning_rate, name='state_value_network'):\n",
    "        self.state_size = state_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "\n",
    "            self.state = tf.placeholder(tf.float32, [None, self.state_size], name=\"state\")\n",
    "            self.advantage_delta = tf.placeholder(tf.float32, name=\"advantage_delta\")\n",
    "            self.gamma_t = tf.placeholder(tf.float32, name=\"gamma_t\")\n",
    "\n",
    "\n",
    "            tf2_initializer = tf.keras.initializers.glorot_normal(seed=0)\n",
    "            self.W1 = tf.get_variable(\"W1\", [self.state_size, 8], initializer=tf2_initializer)\n",
    "            self.b1 = tf.get_variable(\"b1\", [8], initializer=tf2_initializer)\n",
    "            self.W2 = tf.get_variable(\"W2\", [8, 1], initializer=tf2_initializer)\n",
    "            self.b2 = tf.get_variable(\"b2\", [1], initializer=tf2_initializer)\n",
    "\n",
    "            self.Z1 = tf.add(tf.matmul(self.state, self.W1), self.b1)\n",
    "            self.A1 = tf.nn.relu(self.Z1)\n",
    "            self.output = tf.add(tf.matmul(self.A1, self.W2), self.b2)\n",
    "\n",
    "            # Loss calculation  - for gradient ascent we minimize the negative loss. Loss = delta*I*V\n",
    "            self.loss = -self.advantage_delta * self.gamma_t * self.output\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "\n",
    "\n",
    "def run(discount_factor, policy_learning_rate ,sv_learning_rate):\n",
    "    env = gym.make('CartPole-v1')\n",
    "    np.random.seed(SEED)\n",
    "    env.seed(SEED)\n",
    "    tf.set_random_seed(SEED)\n",
    "    rewards, mean_rewards, losses = [], [], []\n",
    "    # Define hyperparameters\n",
    "    state_size = 4\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    max_episodes = 5000\n",
    "    max_steps = 501\n",
    "    discount_factor = discount_factor\n",
    "    policy_learning_rate = policy_learning_rate\n",
    "    sv_learning_rate = sv_learning_rate\n",
    "    render = False\n",
    "\n",
    "    # Initialize the policy and the state-value network\n",
    "    tf.reset_default_graph()\n",
    "    policy = PolicyNetwork(state_size, action_size, policy_learning_rate)\n",
    "    state_value = ValueNetwork(state_size, sv_learning_rate)\n",
    "\n",
    "    # Start training the agent with REINFORCE algorithm\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        solved = False\n",
    "        Transition = collections.namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        episode_rewards = np.zeros(max_episodes)\n",
    "        average_rewards = 0.0\n",
    "        early_stopping = False\n",
    "\n",
    "        for episode in range(max_episodes):\n",
    "            state = env.reset()\n",
    "            state = state.reshape([1, state_size])\n",
    "            episode_transitions = []\n",
    "            since = time.time()\n",
    "            for step in range(max_steps):\n",
    "                actions_distribution = sess.run(policy.actions_distribution, {policy.state: state})\n",
    "                action = np.random.choice(np.arange(len(actions_distribution)), p=actions_distribution)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                next_state = next_state.reshape([1, state_size])\n",
    "\n",
    "                if render:\n",
    "                    env.render()\n",
    "\n",
    "                episode_transitions.append(Transition(state=state, action=action, reward=reward, next_state=next_state, done=done))\n",
    "                episode_rewards[episode] += reward\n",
    "\n",
    "                if done:\n",
    "                    if episode > 98:\n",
    "                        # Check if solved\n",
    "                        average_rewards = np.mean(episode_rewards[(episode - 99):episode+1])\n",
    "\n",
    "                    if np.mean(episode_rewards[(episode - 10):episode + 1]) > 475:\n",
    "                      early_stopping = True\n",
    "\n",
    "                    print(\"Episode {} Reward: {} Average over 100 episodes: {}\".format(episode, episode_rewards[episode], round(average_rewards, 2)))\n",
    "                    if average_rewards > 475:\n",
    "                        print(' Solved at episode: ' + str(episode))\n",
    "                        time_elapsed = time.time() - since\n",
    "                        print(\"Algorithm {} converged after {} seconds\".format(algorithm_name, time_elapsed))\n",
    "                        solved = True\n",
    "                    break\n",
    "                state = next_state\n",
    "\n",
    "            if solved:\n",
    "                break\n",
    "\n",
    "            # Compute Gt for each time-step t and update the network's weights\n",
    "            for t, transition in enumerate(episode_transitions):\n",
    "                # Compute Gt\n",
    "                total_discounted_return = sum(discount_factor ** i * t.reward for i, t in enumerate(episode_transitions[t:]))\n",
    "\n",
    "                # Compute V(S,w)\n",
    "                feed_dict = {state_value.state: transition.state}\n",
    "                V_s = sess.run(state_value.output, feed_dict)\n",
    "\n",
    "                # Compute delta = Gt - V(S,w)\n",
    "                advantage_delta = total_discounted_return - V_s\n",
    "\n",
    "                # Update the state_value network weights\n",
    "                feed_dict = {state_value.state: transition.state, state_value.gamma_t: discount_factor**t,\n",
    "                             state_value.advantage_delta: advantage_delta}\n",
    "                _, loss_state = sess.run([state_value.optimizer, state_value.loss], feed_dict)\n",
    "\n",
    "                # Update the policy network weights\n",
    "                feed_dict = {policy.state: transition.state, policy.gamma_t: discount_factor**t,\n",
    "                             policy.advantage_delta: advantage_delta}\n",
    "\n",
    "                if early_stopping:\n",
    "                # Early stopping to prevent the network weights from changing after it is stable\n",
    "                  loss_policy = sess.run(policy.loss, feed_dict)\n",
    "                else:\n",
    "                  _, loss_policy = sess.run([policy.optimizer, policy.loss], feed_dict)\n",
    "\n",
    "\n",
    "            rewards.append(episode_rewards[episode])\n",
    "            mean_rewards.append(average_rewards)\n",
    "            losses.append(loss_policy)\n",
    "    return episode, rewards, mean_rewards, losses\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    SEED = 42\n",
    "    optimal_sv_lr = 0.0007\n",
    "    optimal_policy_lr = 0.0005\n",
    "    # global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # optimal_policy_lr = tf.train.exponential_decay(optimal_policy_lr,\n",
    "    #                                     global_step, 300,\n",
    "    #                                     0.95, staircase=True)\n",
    "    optimal_df = 0.99\n",
    "    algorithm_name = \"reinforce_wt_baseline\"\n",
    "    for df in [0.9 ,0.95 ,0.99 ,0.995 ,0.9995]:\n",
    "        print(\"df: \", df)\n",
    "        last_episode, rewards, mean_rewards, losses = run(discount_factor=0.99, policy_learning_rate=optimal_policy_lr, sv_learning_rate=optimal_sv_lr)\n",
    "        with open('df_sim_{}_{}.npy'.format(algorithm_name, df), 'wb') as f:\n",
    "            np.save(f, last_episode)\n",
    "            np.save(f, rewards)\n",
    "            np.save(f, mean_rewards)\n",
    "            np.save(f, losses)"
   ]
  }
 ]
}
